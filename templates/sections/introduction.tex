\section{Introduction}

% 
% What is LLM? What is its effect on the whole world?
% 1
\gls{llm} is an unprecedented innovation in many aspects of the world. Moreover, the revolution of \gls{llm} has promoted many efforts on applying \gls{llm} abilities to achieve effectiveness, performance, and efficiency. The adoption of \gls{llm} is very diverse in various applications, such as natural language generation, speech processing, code generation, or image classification~\cite{10.1145/3639372,cui-etal-2025-recent,10.1145/3641289,yin2024survey}.
% 
% What are the effects of LLM on agiel hardware designs?
In fields of engineering, the \gls{llm} usage is very diverse in various procedures ranging from software to hardware~\cite{10.1145/3695988,10876858}. In particular, in hardware engineering, researchers start to explore \gls{llm} capability in hardware design, especially in \gls{rtl} design and verification~\cite{chang2023improving}. Based on the power of \gls{llm}, many potentials are revealed in the automated generation of \gls{rtl} design and testbench~\cite{joel2024survey}.

% 
% Why "Simple" in Verilog Generation?
% Why is "Simple" in Verilog Generation matter?
% Why not Complex task/design in Verilog generation?
Regarding handling complex task in \gls{rtl} design, complicated settings in promting and task decomposition are required \cite{tang2025hivegen}.

% 
%%%%%%%%%%%
% How to adopt LLM in RTL designs?
% 		○ How to use LLM effectively in agiel RTL generation?
% 		○ Is it related to prompting and fine-tuning?
% 		○ How is prompting effective in LLM-powered RTL design?
On using \gls{llm} effectively in agile \gls{rtl} design, the two major approaches are prompt engineering based on \gls{icl}~\cite{lu2024rtllm,gubbi2025prompting,bhattacharyya2024llm,docomba} and instruction fine-tuning~\cite{liu2025rtlcoder,10691738,11133406}. In the prompting approach, immediate effectiveness and performance in syntactic and functional correctness of \gls{rtl} design are easy to achieve by constructing logic definitions and descriptions in the context of generative \gls{llm}, such as GPT-4, Claude 3.5 Sonet, or DeepSeek-R1. Additionally, these generative models with enormous parameter size

% 
% Our contribution


% 
% %%%%%%%%%%%%%%%%%%%%%%%%
% What am I doing?
% Why do I do this stuff?
% Why train this LLM? Why not others?
% Is the DeepSeek Coder a good basis to start from coding's perspective?
% Is it good for instruction tuning for domain-specific tasks in Verilog generation?
% Is the way I fine-tune and customize this meaningful to techniques in constructing an LLM for Verilog generation?
% But what is the root basis to use LLM in such Verilog generation?
% Do the base models perform well in coding, especially in Verilog coding?
% Does it have big efforts in instructing LLM to make it more specific in Verilog generation?
% Are there any bases from existing research papers for this?
% Why does everyone use LLM? Why should and shouldn't they?
% Does LLM facilitate anything in humans' stuff?
% Does LLM boost or make something quickly and effectively?

\gls{llm}~\cite{10.1145/3641289}.

\gls{slm}~\cite{10.1145/3768165}

% 
%%%%%%%%%%%%%%%%%%%%%%
Why use LLM?
\gls{llm} is good at what?
Why not using \gls{llm}?
How does \gls{llm} facilitate generation stuff?
Are \glspl{sllm} able to serve complex tasks, especially in \gls{rtl} design?
Is LLM good? Is it reliable? And why is very enormous to all aspects of the world?
Why is it used many, regularly?
%%% Currently, \glspl{llm} have become an astounding explosion in almost all aspects of the world~\cite{YAO2024100211}. Due to its enormous benefits, many \glspl{llm}, such as ChatGPT, Claude, etc., have been quickly become popular in many aspects of both academia and industry~\cite{fi15060192}.


So, the logic is good to be a consideration for fine-tuning LLM
But are there any bases to perform LLM training based on logic-specific metrics?
Why training based on quality evaluation, from LLM, is not a good consideration?

Why do I have to optimize fine-tuning for low-level logic modules?
Why not train all bunch of Verilog samples in the same scheme, in the same configuration of LoRa fine-tuning?