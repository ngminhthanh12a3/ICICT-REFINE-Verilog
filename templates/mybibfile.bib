@article{docomba,
  title={COMBA-PROMPT: Comprehensive Benchmark and Augmentation for Verilog Generation Leveraging Dual-LLM Prompting},
  author={Nguyen, Vu-Minh-Thanh and Nguyen, Ngoc-Thien-Kim and Le, Duc-Hung},
  journal={Authorea Preprints},
  publisher={Authorea}
}

@INPROCEEDINGS{10691738,
  author={Zhang, Yongan and Yu, Zhongzhi and Fu, Yonggan and Wan, Cheng and Lin, Yingyan Celine},
  booktitle={2024 IEEE LLM Aided Design Workshop (LAD)}, 
  title={MG-Verilog: Multi-grained Dataset Towards Enhanced LLM-assisted Verilog Generation}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  keywords={Codes;Large language models;Conferences;Natural languages;Hardware;Complexity theory;Hardware design languages},
  doi={10.1109/LAD62341.2024.10691738}}

@misc{dettmers2023qloraefficientfinetuningquantized,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.14314}, 
}

@INPROCEEDINGS{11133406,
  author={Nadimi, Bardia and Boutaib, Ghali Omar and Zheng, Hao},
  booktitle={2025 62nd ACM/IEEE Design Automation Conference (DAC)}, 
  title={PyraNet: A Multi-Layered Hierarchical Dataset for Verilog}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  keywords={Codes;Design automation;Accuracy;Large language models;Transformers;Hardware design languages;Large Language Models;fine-tuning;Verilog;dataset;transformers},
  doi={10.1109/DAC63849.2025.11133406}}
@article{taori2023alpaca,
  title={Alpaca: A strong, replicable instruction-following model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html},
  volume={3},
  number={6},
  pages={7},
  year={2023}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Ziqing and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  volume={2},
  number={3},
  pages={6},
  year={2023}
}

@article{guo2024deepseek,
  title={DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence},
  author={Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Yu and Li, YK and others},
  journal={arXiv preprint arXiv:2401.14196},
  year={2024}
}

@article{liu2025rtlcoder,
  title={RTLCoder: Fully Open-Source and Efficient LLM-Assisted RTL Code Generation Technique},
  author={Liu, Shang and Fang, Wenji and Lu, Yao and Wang, Jing and Zhang, Qijun and Zhang, Hongce and Xie, Zhiyao},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume={44},
  number={4},
  pages={1448--1461},
  year={2025},
  publisher={IEEE}
}

@misc{vonwerra2022trl,
	title        = {{TRL: Transformer Reinforcement Learning}},
	author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallou{\'e}dec},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/huggingface/trl}}
}

@misc{chen2025rolesmallmodelsllm,
      title={What is the Role of Small Models in the LLM Era: A Survey}, 
      author={Lihu Chen and Gaël Varoquaux},
      year={2025},
      eprint={2409.06857},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.06857}, 
}

@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@article{YAO2024100211,
title = {A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly},
journal = {High-Confidence Computing},
volume = {4},
number = {2},
pages = {100211},
year = {2024},
issn = {2667-2952},
doi = {https://doi.org/10.1016/j.hcc.2024.100211},
url = {https://www.sciencedirect.com/science/article/pii/S266729522400014X},
author = {Yifan Yao and Jinhao Duan and Kaidi Xu and Yuanfang Cai and Zhibo Sun and Yue Zhang},
keywords = {Large Language Model (LLM), LLM security, LLM privacy, ChatGPT, LLM attacks, LLM vulnerabilities},
abstract = {Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into “The Good” (beneficial LLM applications), “The Bad” (offensive applications), and “The Ugly” (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs’ potential to both bolster and jeopardize cybersecurity.}
}

@Article{fi15060192,
AUTHOR = {Roumeliotis, Konstantinos I. and Tselikas, Nikolaos D.},
TITLE = {ChatGPT and Open-AI Models: A Preliminary Review},
JOURNAL = {Future Internet},
VOLUME = {15},
YEAR = {2023},
NUMBER = {6},
ARTICLE-NUMBER = {192},
URL = {https://www.mdpi.com/1999-5903/15/6/192},
ISSN = {1999-5903},
ABSTRACT = {According to numerous reports, ChatGPT represents a significant breakthrough in the field of artificial intelligence. ChatGPT is a pre-trained AI model designed to engage in natural language conversations, utilizing sophisticated techniques from Natural Language Processing (NLP), Supervised Learning, and Reinforcement Learning to comprehend and generate text comparable to human-generated text. This article provides an overview of the training process and fundamental functionality of ChatGPT, accompanied by a preliminary review of the relevant literature. Notably, this article presents the first comprehensive literature review of this technology at the time of publication, aiming to aggregate all the available pertinent articles to facilitate further developments in the field. Ultimately, the authors aim to offer an appraisal of the technology’s potential implications on existing knowledge and technology, along with potential challenges that must be addressed.},
DOI = {10.3390/fi15060192}
}

@INPROCEEDINGS{10323812,
  author={Liu, Mingjie and Pinckney, Nathaniel and Khailany, Brucek and Ren, Haoxing},
  booktitle={2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)}, 
  title={Invited Paper: VerilogEval: Evaluating Large Language Models for Verilog Code Generation}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  keywords={Codes;Design automation;Benchmark testing;Hardware;Distance measurement;Combinational circuits;Hardware design languages},
  doi={10.1109/ICCAD57390.2023.10323812}}

@article{10.1145/3718088,
author = {Pinckney, Nathaniel and Batten, Christopher and Liu, Mingjie and Ren, Haoxing and Khailany, Brucek},
title = {Revisiting VerilogEval: A Year of Improvements in Large-Language Models for Hardware Code Generation},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {6},
issn = {1084-4309},
url = {https://doi.org/10.1145/3718088},
doi = {10.1145/3718088},
abstract = {The application of large language models (LLMs) to digital hardware code generation is an emerging field, with most LLMs primarily trained on natural language and software code. Hardware code like Verilog constitutes a small portion of training data, and few hardware benchmarks exist. The open-source VerilogEval benchmark, released in November 2023, provided a consistent evaluation framework for LLMs on code completion tasks. Since then, both commercial and open models have seen significant development.In this work, we evaluate new commercial and open models since VerilogEval’s original release—including GPT-4o, GPT-4 Turbo, Llama3.1 (8B/70B/405B), Llama3 70B, Mistral Large, DeepSeek Coder (33B and 6.7B), CodeGemma 7B, and RTL-Coder—against an improved VerilogEval benchmark suite. We find measurable improvements in state-of-the-art models: GPT-4o achieves a 63\% pass rate on specification-to-RTL tasks. The recently released and open Llama3.1 405B achieves a 58\% pass rate, almost matching GPT-4o, while the smaller domain-specific RTL-Coder 6.7B models achieve an impressive 34\% pass rate.Additionally, we enhance VerilogEval’s infrastructure by automatically classifying failures, introducing in-context learning support, and extending the tasks to specification-to-RTL translation. We find that prompt engineering remains crucial for achieving good pass rates and varies widely with model and task. A benchmark infrastructure that allows for prompt engineering and failure analysis is essential for continued model development and deployment.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = oct,
articleno = {91},
numpages = {20},
keywords = {Large language models, RTL code generation, benchmarks}
}

@online{gpt4o,
	title={Hello GPT-4o},
    author={OpenAI},
	url={https://openai.com/index/hello-gpt-4o/},
	urldate = {2024-12-19},
    year = {2024},
    date = {2024-05-13}
}

@online{gpt4_turbo_announce,
    author = {OpenAI},
	title = {New models and developer products announced at {DevDay}},
	url = {https://openai.com/index/new-models-and-developer-products-announced-at-devday/},
	abstract = {{GPT}-4 Turbo with 128K context and lower prices, the new Assistants {API}, {GPT}-4 Turbo with Vision, {DALL}·E 3 {API}, and more.},
    date = {2023-11-06},
    year = {2023},
	urldate = {2024-05-28},
	langid = {american},
	file = {Snapshot:/Users/npfet/Zotero/storage/XVZXPLB9/new-models-and-developer-products-announced-at-devday.html:text/html},
}

@misc{gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@online{ai_au_2024,
	title = {Au Large},
	url = {https://mistral.ai/news/mistral-large/},
	abstract = {Mistral Large is our flagship model, with top-tier reasoning capacities. It is also available on Azure.},
	author = {Mistral AI},
	urldate = {2024-05-31},
	date = {2024-02-26},
	langid = {english},
	note = {Section: news},
	file = {Snapshot:/Users/npfet/Zotero/storage/QQWNCVZ9/mistral-large.html:text/html},
    year = {2024},

}

@online{noauthor_meta-llamallama3_2024,
    author = {Meta},
 	title = {meta-llama/llama-models},
	url = {https://github.com/meta-llama/llama-models},
	abstract = {Utilities intended for use with Llama models.},
	publisher = {Meta Llama},
	urldate = {2024-08-16},
	date = {2024-08-16},
	note = {original-date: 2024-06-27T22:14:09Z},
    year = {2024}
}

@online{noauthor_meta-llamacodellama-70b-instruct-hf_2024,
    author = {Meta},
	title = {meta-llama/{CodeLlama}-70b-Instruct-hf · Hugging Face},
	url = {https://huggingface.co/meta-llama/CodeLlama-70b-Instruct-hf},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-05-28},
	date = {2024-04-18},
}

@misc{guo2024deepseekcoder,
      title={DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence}, 
      author={Daya Guo and Qihao Zhu and Dejian Yang and Zhenda Xie and Kai Dong and Wentao Zhang and Guanting Chen and Xiao Bi and Y. Wu and Y. K. Li and Fuli Luo and Yingfei Xiong and Wenfeng Liang},
      year={2024},
      eprint={2401.14196},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@online{noauthor_googlecodegemma-7b_2024,
    author = {Google},
	title = {google/codegemma-7b · Hugging Face},
	url = {https://huggingface.co/google/codegemma-7b},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-05-28},
	date = {2024-05-14},
	file = {Snapshot:/Users/npfet/Zotero/storage/UVC6G9CF/codegemma-7b.html:text/html},
}

@article{10.1145/3768165,
author = {Wang, Fali and Zhang, Zhiwei and Zhang, Xianren and Wu, Zongyu and Mo, TzuHao and Lu, Qiuhao and Wang, Wanjing and Li, Rui and Xu, Junjie and Tang, Xianfeng and He, Qi and Ma, Yao and Huang, Ming and Wang, Suhang},
title = {A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3768165},
doi = {10.1145/3768165},
abstract = {Large language models (LLMs) have demonstrated emergent abilities in text generation, question answering, and reasoning, facilitating various tasks and domains. Despite their proficiency in various tasks, LLMs like PaLM 540B and Llama-3.1 405B face limitations due to large parameter sizes and computational demands, often requiring cloud API use, which raises privacy concerns, limits real-time applications on edge devices, and increases fine-tuning costs. Additionally, LLMs often underperform in specialized domains such as healthcare and law due to insufficient domain-specific knowledge, necessitating specialized models. Therefore, Small Language Models (SLMs) are increasingly favored for their low inference latency, cost-effectiveness, efficient development, and easy customization and adaptability. These models are particularly well-suited for resource-limited environments and domain knowledge acquisition, addressing LLMs’ challenges and proving ideal for applications that require localized data handling for privacy, minimal inference latency for efficiency, and domain knowledge acquisition through lightweight fine-tuning. The rising demand for SLMs has spurred extensive research and development. However, a comprehensive survey investigating issues related to the definition, acquisition, application, enhancement, and reliability of SLM remains lacking, prompting us to conduct a detailed survey on these topics. The definition of SLMs varies widely; thus, to standardize, we propose defining SLMs by their capability to perform specialized tasks and suitability for resource-constrained settings, setting boundaries based on the minimal size for emergent abilities and the maximum size sustainable under resource constraints. For other aspects, we provide a taxonomy of relevant models/methods and develop general frameworks for each category to enhance and utilize SLMs effectively. We have compiled the collected SLM models and related methods on GitHub: .},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {145},
numpages = {87},
keywords = {Small Language Models, On-Device LLMs, Domain-specific Models, Trustworthiness}
}

@article{10.1145/3641289,
author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
title = {A Survey on Evaluation of Large Language Models},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3641289},
doi = {10.1145/3641289},
abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at:},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {39},
numpages = {45},
keywords = {Large language models, evaluation, model assessment, benchmark}
}

@article{10.1145/3639372,
author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
title = {Explainability for Large Language Models: A Survey},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3639372},
doi = {10.1145/3639372},
abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this article, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {20},
numpages = {38},
keywords = {Explainability, interpretability, large language models}
}

@inproceedings{cui-etal-2025-recent,
    title = "Recent Advances in Speech Language Models: A Survey",
    author = "Cui, Wenqian  and
      Yu, Dianzhi  and
      Jiao, Xiaoqi  and
      Meng, Ziqiao  and
      Zhang, Guangyan  and
      Wang, Qichao  and
      Guo, Steven Y.  and
      King, Irwin",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.682/",
    doi = "10.18653/v1/2025.acl-long.682",
    pages = "13943--13970",
    ISBN = "979-8-89176-251-0",
    abstract = "Text-based Large Language Models (LLMs) have recently gained significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, highlighting the need for voice-based models. In this context, Speech Language Models (SpeechLMs){---}foundation models designed to understand and generate speech{---}emerge as a promising solution for end-to-end speech interaction. This survey offers a comprehensive overview of recent approaches to building SpeechLMs, outlining their core architectural components, training methodologies, evaluation strategies, and the challenges and potential directions for future research in this rapidly advancing field. The GitHub repository is available at https://github.com/dreamtheater123/Awesome-SpeechLM-Survey"
}

@article{yin2024survey,
  title={A survey on multimodal large language models},
  author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
  journal={National Science Review},
  volume={11},
  number={12},
  pages={nwae403},
  year={2024},
  publisher={Oxford University Press}
}

@article{10.1145/3695988,
author = {Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
title = {Large Language Models for Software Engineering: A Systematic Literature Review},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3695988},
doi = {10.1145/3695988},
abstract = {Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a Systematic Literature Review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We selected and analyzed 395 research articles from January 2017 to January 2024 to answer four key Research Questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, pre-processing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and highlighting promising areas for future study. Our artifacts are publicly available at .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {220},
numpages = {79},
keywords = {Software Engineering, Large Language Model, Survey}
}

@ARTICLE{10876858,
  author={Guo, Cong and Cheng, Feng and Du, Zhixu and Kiessling, James and Ku, Jonathan and Li, Shiyu and Li, Ziru and Ma, Mingyuan and Molom-Ochir, Tergel and Morris, Benjamin and Shan, Haoxuan and Sun, Jingwei and Wang, Yitu and Wei, Chiyue and Wu, Xueying and Wu, Yuhao and Yang, Hao Frank and Zhang, Jingyang and Zhang, Junyao and Zheng, Qilin and Zhou, Guanglei and Li, Hai and Chen, Yiran},
  journal={IEEE Circuits and Systems Magazine}, 
  title={A Survey: Collaborative Hardware and Software Design in the Era of Large Language Models}, 
  year={2025},
  volume={25},
  number={1},
  pages={35-57},
  keywords={Large language models;Artificial intelligence;Natural language processing;Multimodal sensors;Energy consumption;Deep learning;Training;Hardware design languages;Algorithm design and analysis;Software algorithms;Inference algorithms;Next generation networking;Collaborative software;Large language model;hardware-software co-design},
  doi={10.1109/MCAS.2024.3476008}}

@article{chang2023improving,
  title={Improving large language model hardware generating quality through post-llm search},
  author={Chang, Kaiyan and Ren, Haimeng and Wang, Mengdi and Liang, Shengwen and Han, Yinhe and Li, Huawei and Li, Xiaowei and Wang, Ying},
  journal={Proceedings of the Machine Learning for Systems},
  year={2023}
}

@inproceedings{lu2024rtllm,
  title={Rtllm: An open-source benchmark for design rtl generation with large language model},
  author={Lu, Yao and Liu, Shang and Zhang, Qijun and Xie, Zhiyao},
  booktitle={2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC)},
  pages={722--727},
  year={2024},
  organization={IEEE}
}

@article{joel2024survey,
  title={A survey on llm-based code generation for low-resource and domain-specific programming languages},
  author={Joel, Sathvik and Wu, Jie and Fard, Fatemeh},
  journal={ACM Transactions on Software Engineering and Methodology},
  year={2024},
  publisher={ACM New York, NY}
}

@inproceedings{gubbi2025prompting,
  title={Prompting for Power: Benchmarking Large Language Models for Low-Power RTL Design Generation},
  author={Gubbi, Kevin Immanuel and Halm, Marcus and Kumar, Sarbani and Sudarshan, Arvind and Kota, Pavan Dheeraj and Tarighat, Mohammadnavid and Sasan, Avesta and Homayoun, Houman},
  booktitle={2025 ACM/IEEE 7th Symposium on Machine Learning for CAD (MLCAD)},
  pages={1--7},
  year={2025},
  organization={IEEE}
}

@inproceedings{bhattacharyya2024llm,
  title={LLM vs HLS for RTL Code Generation: Friend or Foe?},
  author={Bhattacharyya, Sutirtha and Sutharshanan, BG and Karfa, Chandan},
  booktitle={2024 IEEE 33rd Asian Test Symposium (ATS)},
  pages={1--6},
  year={2024},
  organization={IEEE}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{tang2025hivegen,
  title={Hivegen--hierarchical llm-based verilog generation for scalable chip design},
  author={Tang, Jinwei and Qin, Jiayin and Thorat, Kiran and Zhu-Tian, Chen and Cao, Yu and Zhao, Yang Katie and Ding, Caiwen},
  booktitle={2025 IEEE International Conference on LLM-Aided Design (ICLAD)},
  pages={30--36},
  year={2025},
  organization={IEEE}
}